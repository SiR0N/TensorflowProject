{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"notMNIST.pickle\"\n",
    "def make_datasets (file, n_training_samples=0, n_dev_samples=0, \n",
    "                   n_testing_samples=0, one_hot=False):\n",
    "    with open (file,'rb') as f:\n",
    "        dataset = pickle.load(f,encoding='latin1')\n",
    "        f.close\n",
    "\n",
    "    train_dataset = dataset['train_dataset']\n",
    "    train_labels = dataset['train_labels']\n",
    "    dev_dataset = dataset['valid_dataset']\n",
    "    dev_labels = dataset['valid_labels']\n",
    "    test_dataset = dataset['test_dataset']\n",
    "    test_labels = dataset['test_labels']\n",
    "\n",
    "    #Prepare training, dev (validation) and final testing data. \n",
    "    #It has to be reshaped since (n_samples, n_fatures) are expected\n",
    "\n",
    "    all_training_samples, width, height = train_dataset.shape\n",
    "    train_attributes = np.reshape(train_dataset, (all_training_samples, \n",
    "                                                  width * height))\n",
    "    if (n_training_samples != 0):\n",
    "        train_attributes = train_attributes[0:n_training_samples]\n",
    "        train_labels = train_labels[0:n_training_samples]\n",
    "\n",
    "    all_dev_samples, width, height = dev_dataset.shape\n",
    "    dev_attributes = np.reshape(dev_dataset,\n",
    "                                       (all_dev_samples, width * height))\n",
    "    if (n_dev_samples != 0):\n",
    "        dev_attributes = dev_attributes[0:n_dev_samples]\n",
    "        dev_labels = dev_labels[0:n_dev_samples]\n",
    "\n",
    "    all_testing_samples, width, height = test_dataset.shape\n",
    "    test_attributes = np.reshape(test_dataset, (all_testing_samples, width * height))\n",
    "    if (n_testing_samples != 0):\n",
    "        test_attributes = test_attributes[0:n_testing_samples]\n",
    "        test_labels = test_labels[0:n_testing_samples]\n",
    "\n",
    "    # If one-hot encoding is requested, then funtion OneHotEcoding \n",
    "    # from SciKit-Learn is called    \n",
    "    if one_hot:\n",
    "        enc = OneHotEncoder(sparse=False)\n",
    "        # Labels are one-dimensional vectors, \n",
    "        # and are reshaped to matrices of one column\n",
    "        train_labels = enc.fit_transform(train_labels.reshape(len(train_labels),1))\n",
    "        dev_labels = enc.fit_transform(dev_labels.reshape(len(dev_labels), 1))\n",
    "        test_labels = enc.fit_transform(test_labels.reshape(len(test_labels), 1))\n",
    "\n",
    "    return (train_attributes, train_labels, dev_attributes, \n",
    "            dev_labels, test_attributes, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRAINING_SAMPLES = 10000\n",
    "NUM_DEV_SAMPLES = 1000\n",
    "NUM_TESTING_SAMPLES = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train, y_train, x_dev, y_dev, x_test, y_test = make_datasets(file_name, \n",
    "                                 n_training_samples=NUM_TRAINING_SAMPLES,\n",
    "                                 n_dev_samples=NUM_DEV_SAMPLES, \n",
    "                                 n_testing_samples=NUM_TESTING_SAMPLES,\n",
    "                                 one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the deep neural network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper-paramenters configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10000\n",
    "epochs_to_display = 500\n",
    "batch_size = 200\n",
    "initial_learning_rate = 0.0075\n",
    "\n",
    "n_inputs = len(x_train[0])\n",
    "#Fibonacci numbers: 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 1.597   \n",
    "n_hidden1 = 377\n",
    "n_hidden2 = 233\n",
    "n_hidden3 = 144\n",
    "n_hidden4 = 89\n",
    "n_hidden5 = 55\n",
    "n_hidden6 = 34\n",
    "n_hidden7 = 21\n",
    "\n",
    "n_outputs = len(y_train[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the input __X__ and target __t__ matrices are defined as placeholders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"io\"):\n",
    "    X = tf.placeholder(dtype=tf.float32, shape=(None,n_inputs), name=\"X\")\n",
    "    t = tf.placeholder(dtype=tf.float32, shape=(None,n_outputs), name=\"t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the neural network topology is defined: A full-connected 28x28-300-200-100-10 deep neural network. Note that ReLU is the activation function for the hidden layers, and linear logits with softmax for the output. net_out represents the logits of the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "training= tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "dropout_rate_big = 0.40\n",
    "dropout_rate_small = 0.25\n",
    "                                      \n",
    "X_drop = tf.layers.dropout(X, dropout_rate_small, training=training)\n",
    "\n",
    "def leaky_relu(net, name=None):\n",
    "    return tf.maximum(0.01 * net, net, name=name)\n",
    "\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    #hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\",reuse=tf.AUTO_REUSE,kernel_initializer = he_init)\n",
    "    hidden1 = tf.layers.dense(X_drop, n_hidden1, activation=leaky_relu, name=\"hidden1\",reuse=tf.AUTO_REUSE,kernel_initializer = he_init)\n",
    "    hidden1_drop = tf.layers.dropout(hidden1, dropout_rate_big, training=training)\n",
    "                                      \n",
    "    #hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\",reuse=tf.AUTO_REUSE, kernel_initializer = he_init)\n",
    "    hidden2 = tf.layers.dense(hidden1_drop, n_hidden2, activation=leaky_relu, name=\"hidden2\",reuse=tf.AUTO_REUSE, kernel_initializer = he_init)\n",
    "    hidden2_drop = tf.layers.dropout(hidden2, dropout_rate_big, training=training)\n",
    "                                      \n",
    "    #hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\",reuse=tf.AUTO_REUSE, kernel_initializer = he_init)\n",
    "    hidden3 = tf.layers.dense(hidden2_drop, n_hidden3, activation=leaky_relu, name=\"hidden3\",reuse=tf.AUTO_REUSE, kernel_initializer = he_init)\n",
    "    hidden3_drop = tf.layers.dropout(hidden3, dropout_rate_big, training=training)\n",
    "    \n",
    "    hidden4 = tf.layers.dense(hidden3_drop, n_hidden4, activation=leaky_relu, name=\"hidden4\",reuse=tf.AUTO_REUSE, kernel_initializer = he_init)\n",
    "    hidden4_drop = tf.layers.dropout(hidden4, dropout_rate_small, training=training)\n",
    "    \n",
    "    hidden5 = tf.layers.dense(hidden4_drop, n_hidden5, activation=leaky_relu, name=\"hidden5\",reuse=tf.AUTO_REUSE, kernel_initializer = he_init)\n",
    "    hidden5_drop = tf.layers.dropout(hidden5, dropout_rate_small, training=training)\n",
    "    \n",
    "    hidden6 = tf.layers.dense(hidden5_drop, n_hidden6, activation=leaky_relu, name=\"hidden6\",reuse=tf.AUTO_REUSE, kernel_initializer = he_init)\n",
    "    hidden6_drop = tf.layers.dropout(hidden6, dropout_rate_small, training=training)\n",
    "    \n",
    "    hidden7 = tf.layers.dense(hidden6_drop, n_hidden7, activation=leaky_relu, name=\"hidden7\",reuse=tf.AUTO_REUSE, kernel_initializer = he_init)\n",
    "    hidden7_drop = tf.layers.dropout(hidden7, dropout_rate_small, training=training)\n",
    "                                      \n",
    "    net_out = tf.layers.dense(hidden7_drop, n_outputs, name=\"net_out\",reuse=tf.AUTO_REUSE)\n",
    "    y = tf.nn.softmax(logits=net_out, name=\"y\")\n",
    "    rounded_y = tf.round(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss and cost functions with cross entropy and log-loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-b4b229523c05>:2: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=t, logits=net_out)\n",
    "    mean_log_loss = tf.reduce_mean(cross_entropy, name=\"mean_loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the learning algorithm: gradient descent with back-prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Momentum\n",
    "#decay_steps = 10000\n",
    "#decay_rate = 0.80\n",
    "#global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "#learning_rate = tf.train.exponential_decay(initial_learning_rate,global_step, decay_steps,decay_rate)\n",
    "#optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.95, use_nesterov=True)\n",
    "#train_step = optimizer.minimize(mean_log_loss, global_step=global_step)\n",
    "\n",
    "#Adam\n",
    "decay_momentum = 0.875\n",
    "decay_scaling = 0.99898989898\n",
    "epsilon = 6.626e-4   #Planck constant: 6.626069934(89)×10−34\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=initial_learning_rate,\n",
    "                                   beta1=decay_momentum,\n",
    "                                   beta2=decay_scaling,\n",
    "                                   epsilon=epsilon,)\n",
    "train_step = optimizer.minimize(mean_log_loss)\n",
    "\n",
    "#Orig\n",
    "#train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(mean_log_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_predictions = tf.equal(tf.argmax(y, 1), tf.argmax(t, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_predictions,tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters:\n",
      " Batch size:  200 \n",
      " Initial learning rate:  0.0075 \n",
      " Neurons in layer 1:  377 \n",
      " Neurons in layer 2:  233 \n",
      " Neurons in layer 3:  144 \n",
      " Neurons in layer 4:  89 \n",
      " Neurons in layer 5:  55 \n",
      " Neurons in layer 6:  34 \n",
      " Neurons in layer 7:  21 \n",
      " Activation: leaky_relu \n",
      " Drop rate 'big':  0.4 \n",
      " Drop rate 'small':  0.25 \n",
      " Decay M:  0.875 \n",
      " Decay S:  0.99898989898 \n",
      " Epsilon:  0.0006626 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "print(\"Parameters:\\n\",\n",
    "    \"Batch size: \",batch_size,\"\\n\",\n",
    "    \"Initial learning rate: \",initial_learning_rate,\"\\n\",\n",
    "    \"Neurons in layer 1: \",n_hidden1,\"\\n\",\n",
    "    \"Neurons in layer 2: \",n_hidden2,\"\\n\",\n",
    "    \"Neurons in layer 3: \",n_hidden3,\"\\n\",\n",
    "    \"Neurons in layer 4: \",n_hidden4,\"\\n\",\n",
    "    \"Neurons in layer 5: \",n_hidden5,\"\\n\",\n",
    "    \"Neurons in layer 6: \",n_hidden6,\"\\n\",\n",
    "    \"Neurons in layer 7: \",n_hidden7,\"\\n\",\n",
    "    \"Activation: leaky_relu\",\"\\n\",\n",
    "    \"Drop rate 'big': \",dropout_rate_big,\"\\n\",\n",
    "    \"Drop rate 'small': \",dropout_rate_small,\"\\n\",\n",
    "    \"Decay M: \",decay_momentum,\"\\n\",\n",
    "    \"Decay S: \",decay_scaling,\"\\n\",\n",
    "    \"Epsilon: \",epsilon,\"\\n\"  \n",
    "    #\"Decay rate: \",decay_rate,\"\\n\",\n",
    "    #\"Momentum: 0.95\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 Train accuracy:  0.9412 Development accuracy:  0.862\n",
      "1000 Train accuracy:  0.9816 Development accuracy:  0.862\n",
      "1500 Train accuracy:  0.9923 Development accuracy:  0.863\n",
      "2000 Train accuracy:  0.9949 Development accuracy:  0.868\n",
      "2500 Train accuracy:  0.9956 Development accuracy:  0.867\n",
      "3000 Train accuracy:  0.9963 Development accuracy:  0.868\n",
      "3500 Train accuracy:  0.9967 Development accuracy:  0.865\n",
      "4000 Train accuracy:  0.9965 Development accuracy:  0.874\n",
      "4500 Train accuracy:  0.9975 Development accuracy:  0.867\n",
      "5000 Train accuracy:  0.9968 Development accuracy:  0.871\n",
      "5500 Train accuracy:  0.9973 Development accuracy:  0.871\n",
      "6000 Train accuracy:  0.9973 Development accuracy:  0.875\n",
      "6500 Train accuracy:  0.9974 Development accuracy:  0.872\n",
      "7000 Train accuracy:  0.9975 Development accuracy:  0.874\n",
      "7500 Train accuracy:  0.9977 Development accuracy:  0.872\n",
      "8000 Train accuracy:  0.9974 Development accuracy:  0.868\n",
      "8500 Train accuracy:  0.9976 Development accuracy:  0.867\n",
      "9000 Train accuracy:  0.9976 Development accuracy:  0.878\n",
      "9500 Train accuracy:  0.9975 Development accuracy:  0.878\n",
      "10000 Train accuracy:  0.9978 Development accuracy:  0.88\n",
      "Test accuracy:  0.921\n",
      "Target values:\n",
      " [[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]] \n",
      "Computed values:\n",
      " [[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "First 100 Predictions:  [ True  True  True  True  True  True  True  True False  True False  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True False  True  True  True\n",
      "  True  True  True  True  True  True  True  True False  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True False  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True]\n",
      "Elapsed time:  645.7613933086395 secs.\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "\n",
    "#extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range (int(n_epochs / epochs_to_display)):\n",
    "        for iteration in range (epochs_to_display):\n",
    "            offset = (iteration * epoch * batch_size) % (y_train.shape[0] - batch_size)\n",
    "            sess.run(train_step, feed_dict={training: True, X: x_train, t: y_train})\n",
    "            #sess.run([train_step,extra_update_ops], feed_dict={training: True, X: x_train, t: y_train})\n",
    "            \n",
    "        accuracy_train = accuracy.eval(feed_dict={training: False, X: x_train, t: y_train})\n",
    "        accuracy_dev = accuracy.eval(feed_dict={training: False, X: x_dev, t: y_dev})\n",
    "        print((epoch+1)*epochs_to_display, \"Train accuracy: \", accuracy_train, \n",
    "              \"Development accuracy: \", accuracy_dev)\n",
    "\n",
    "    accuracy_test = accuracy.eval(feed_dict={X: x_test, t: y_test})\n",
    "    print (\"Test accuracy: \", accuracy_test)\n",
    "    \n",
    "    print (\"Target values:\\n\", y_test[0:10], \"\\nComputed values:\\n\", \n",
    "           rounded_y.eval(feed_dict={X: x_test[0:10]}))\n",
    "    print (\"First 100 Predictions: \", \n",
    "           correct_predictions.eval(feed_dict={X: x_test[0:100], t: y_test[0:100]}))\n",
    "print (\"Elapsed time: \", time()-start_time, \"secs.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
